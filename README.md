# AI-Law-Minicourse-HW

<h1>Supreme Court Topic Modeling Analysis</h1> 

1. Step 1 - Data Collection & Preparation.ipynb
Before we can analyze the content of supreme court cases and their relationships with other cases, we need a dataset that containing the text of all cases. First, the code assembles a list of all supreme court cases within a chosen period (here 1760 to 2018) and collects the URLs for each of these cases from casetext.com.

2. Step 2 - Data Collection & Preparation.ipynb
Next, the program uses the case names and URLs from step one to pull the text of each case from casetext.com. Requests are split into smaller chunks so that the website does not block the IP address of the computer executing the code. By the end of step 2, the program has assembled a table containing, for each supreme court case, the case’s URL, title, year, and decision text. 

3. Step 3 - Data Processing.ipynb
Not all words contained within each supreme court decision carry significance when trying to separate cases by topic. So the program removes words and characters that will not be useful in learning about the relationships between cases and assigning topics to these relationships (which would also likely slow down processing). Case names, people’s names, place names and geographical indicators, date indicators such as months, and some law specific words with only procedural significance such as ‘court’ or ‘writ’ are removed from the dataset. By the end of step 3, our dataset consists of case URLs, titles, years, and ‘cleaned’ text for each case. 

4. Step 4 - Topic Modeling Method Testing.ipynb
Now that our data are assembled and cleaned, we need to chose which model will be used to group related words within cases together and establish our topics. The program tests three topic models: LDA, LSA, NMF to try to find which is generates the most useful topic identities. Each of these models is implemented by using the Sci-Kit library, which has functions NMF, LatentDirichletAllocation, and TruncatedSVD. Applying the 3 models to our dataset, it is clear that the NMF model works best--the topics (clusters of associated words) generated by the LDA and LSA models appear to have less in common than the topics generated by the NMF model.

5. Step 5 - Topic Model Application to Data.ipynb
Finally, the program applies NMF to the data collected in steps 1-3 to create find clusters of words that are associated with each other. From these groups of words, the program assigns topic titles, and creates a dictionary that maps any particular topic title to the words (or the numeric codes assigned to those words). The program then generates a reverse dictionary that maps words to topics. Using this reverse dictionary, the program assigns topics to each case, and ultimately visualizes topics as they appear in cases over the years in a graph.

<h2>Tensorflow word2vec Tutorial Analysis</h2>

1. Step 1 - Data Collection & Preparation 
First, we need a data set to train our model. After installing the required modules, the function maybe_download downloads a data file from the web which contains a large set of english sentences. read_data extracts zip file as string.

2. Step 2 - Data Processing
The next step is building the dictionary. The program selects the 50,000 most commonly used the words in the downloaded data file and assigns each word a numeric code. The program creates a dictionary relating each of the most common words to its code, and another dictionary relating these codes back to their associated word.

3. Step 3 - Creating Training Data
Because it would be computationally expensive to feed the entire data set to our model at once, we need to break it into smaller chunks first. So the program generates batches of data to feed into the TensorFlow algorithm. The code specifically tells the machine to go back through a little before the end of the batch to avoid skipping words at the end of a batch. This ensures a more complete analysis.

4. Step 4 - Building and Training the Model 
This step creates the model that we will train and selects the parameters for our model (how many words in each direction we will look at, how big our small batches will be). We also set up the a validation procedure by randomly picking some of the most common words from the original data set and monitoring which other words the model associates with them. The program also computes the average NCE loss for the batch to remove biases. 

5. Step 5 - Training the Model 
With these parameters set, we train the model defined in step 4 using the small chunks of text that we generated in step 3. The program regularly prints the words closely associated to our randomly chosen validation words so we can see how it is doing. After feeding the model 10,000 chunks of words, we have the final_embeddings which represents which words are relatively 'close' to each other.

6. Step 6 - Visualization
To visualize which words the model thinks are similar, the program tells the machine to create a plot graphing the 500 most common words based on the TensorFlow results-- 'similar' words (contextually) appear close to each other. The plot helps the user see how the words are associated with each other based on similarities.